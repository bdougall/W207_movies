{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9533b06-7110-418e-9472-fc5b5117f953",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8394217b-7ec5-47b9-92ab-fc212d6c00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7dd9b-c232-4dc4-bba3-f3b3d4576d55",
   "metadata": {},
   "source": [
    "### Step 2: Define working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab05abd0-103d-4964-96ab-b799df715b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data = 'C:/Users/lbros/Documents/MIDS/W207/final_project/raw_data/'\n",
    "path_clean_data = 'C:/Users/lbros/Documents/MIDS/W207/final_project/clean_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826754a3-e66a-490a-9394-c965033b75a2",
   "metadata": {},
   "source": [
    "### Step 3: Load clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d2d-65bd-4909-8025-b53a7a472b0b",
   "metadata": {},
   "source": [
    "#### Split ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e3572a-528c-4b60-8958-770842618794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ratings dataframes\n",
    "dev_train = pd.read_csv(path_clean_data + 'dev_train.csv')\n",
    "dev_test = pd.read_csv(path_clean_data + 'dev_test.csv')\n",
    "test_train = pd.read_csv(path_clean_data + 'test_train.csv')\n",
    "test_test = pd.read_csv(path_clean_data + 'test_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ed7d75-a2e1-414a-a2aa-5b5e6b453a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'Unnamed 0' column\n",
    "dev_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dev_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_test.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ecbdef-1d61-4601-8c73-0f34ca00b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "dev_train: (906733, 3)\n",
      "dev_test: (5000, 3)\n",
      "test_train: (19545941, 3)\n",
      "test_test: (110715, 3)\n"
     ]
    }
   ],
   "source": [
    "# print dataframes shapes\n",
    "print('Shapes')\n",
    "print('dev_train:', dev_train.shape)\n",
    "print('dev_test:', dev_test.shape)\n",
    "print('test_train:', test_train.shape)\n",
    "print('test_test:', test_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6bcb1c-5543-40d3-9f41-83189a8d20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print dataframes columns\n",
    "print(dev_train.columns)\n",
    "print(dev_test.columns)\n",
    "print(test_train.columns)\n",
    "print(test_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6738b2-7106-4798-88ae-ebbc4764507b",
   "metadata": {},
   "source": [
    "#### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33937f1e-11ba-44b4-815c-348797407cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load movies dataframe\n",
    "movies_df = pd.read_csv(path_clean_data + 'movies_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92f1af8-da50-45bb-b958-cdb87cb47634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set imdb_id as index\n",
    "movies_df.set_index('imdb_id', verify_integrity=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5981321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of movies with unknown tagline: 0.5505\n",
      "Proportion of movies with unknown overview: 0.0213\n",
      "Proportion of movies with unknown description: 0.3161\n",
      "Proportion of movies with unknown production companies or countries: 0.2719\n",
      "Proportion of movies with unknown cast or crew names: 0.0609\n"
     ]
    }
   ],
   "source": [
    "# Find the proportion of movies with an unknown tagline\n",
    "# Since more than half of movies in the dataset don't have a tagline, we'll drop this field\n",
    "print('Proportion of movies with unknown tagline: {:.4f}'.format(len(movies_df[movies_df.tagline == 'unknown'])/(len(movies_df))))\n",
    "\n",
    "# Find the proportion of movies with unknown overview - only about 2% of movies are missing this field\n",
    "print('Proportion of movies with unknown overview: {:.4f}'.format(len(movies_df[movies_df.overview == 'unknown'])/(len(movies_df))))\n",
    "\n",
    "# Find the proportion of movies with unknown description - ~ one-third of movies don't have a result for this field\n",
    "# Since the overview field also contains a description, but contains fewer missing values, we'll use this field\n",
    "print('Proportion of movies with unknown description: {:.4f}'.format(len(movies_df[movies_df.description == 'unknown'])/(len(movies_df))))\n",
    "\n",
    "# Find the proportion of movies without production companies or production countries listed\n",
    "# Nearly a third of films in the dataset don't have a value for production companies or production countries\n",
    "# We'll drop these fields\n",
    "print('Proportion of movies with unknown production companies or countries: {:.4f}'.format(len(\n",
    "    (movies_df[(movies_df.production_companies == 'unknown') | (movies_df.production_countries == 'unknown')]))\n",
    "                                                     /len(movies_df)))\n",
    "\n",
    "# Only a small proportion of films are missing cast or crew names, so we'll keep these fields\n",
    "print('Proportion of movies with unknown cast or crew names: {:.4f}'.format(len(\n",
    "    (movies_df[(movies_df.cast_names == 'unknown') | (movies_df.crew_names == 'unknown')]))\n",
    "                                                     /len(movies_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44445309-34b5-4d5f-8426-07c260c3ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove fields unnamed: 0, id, tagline, description, production_countries, production_companies\n",
    "movies_df = movies_df.drop(['Unnamed: 0', 'id', 'tagline', \n",
    "                            'description', 'production_countries', 'production_companies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fbcf4b-a9ed-4a4e-9fdc-ca6cd56cdd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44628, 173)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe shape\n",
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e287c09-ab07-476e-b8c1-541cb3fd5899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adult', 'belongs_to_collection', 'budget', 'originally_english',\n",
       "       'overview', 'popularity', 'revenue', 'runtime', 'title', 'video',\n",
       "       ...\n",
       "       'zh', 'zu', 'canceled', 'in-production', 'planned', 'post-production',\n",
       "       'released', 'rumored', 'cast_names', 'crew_names'],\n",
       "      dtype='object', length=173)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe columns\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d973a-0c48-4a18-b8cc-bc6777cd297f",
   "metadata": {},
   "source": [
    "### Step 4: Vectorize text fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbb556",
   "metadata": {},
   "source": [
    "Since we had insufficient memory to keep all text features from all fields, we chose to limit the features retained from transforming the text features in the literature to a subset of features. Based upon examples in the literature, we chose to keep the top 200 most significant words to the overview and title fields. We elected to further narrow the crew and cast names to the 100 most significant crew and cast names respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4008eb35-3136-45e9-8914-805f89539194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_df shape:  (44628, 769)\n"
     ]
    }
   ],
   "source": [
    "# create an instance of a TfidfVectorizer object for overview\n",
    "# set it to keep the top 200 most significant words in the overview column\n",
    "tfidf_overview = TfidfVectorizer(max_features=200)\n",
    "t_overview = tfidf_overview.fit_transform(movies_df.overview)\n",
    "\n",
    "# create a dataframe of the transformed top 200 overview features\n",
    "overview = pd.DataFrame(t_overview.todense()).add_prefix('overview_')\n",
    "\n",
    "# fill the NA values in the title column with unknown\n",
    "movies_df.title.fillna('unknown', inplace=True)\n",
    "\n",
    "# create an instance of a TfidfVectorizer object for title\n",
    "# set it to keep the top 200 most significant words in the title column\n",
    "tfidf_title = TfidfVectorizer(max_features=200)\n",
    "t_title = tfidf_title.fit_transform(movies_df.title)\n",
    "\n",
    "# create a dataframe of the transformed top 200 overview features\n",
    "title = pd.DataFrame(t_title.todense()).add_prefix('title_')\n",
    "\n",
    "# create an instance of a TfidfVectorizer object for cast names, keeping the 100 most significant cast names\n",
    "tfidf_cast = TfidfVectorizer(max_features=100)\n",
    "t_cast = tfidf_cast.fit_transform(movies_df.cast_names)\n",
    "\n",
    "# create a dataframe of the transformed top 100 cast name features\n",
    "cast = pd.DataFrame(t_cast.todense()).add_prefix('cast_')\n",
    "\n",
    "# create an instance of a TfidfVectorizer for crew names, keeping the 100 most significant crew names\n",
    "tfidf_crew = TfidfVectorizer(max_features=100)\n",
    "t_crew = tfidf_crew.fit_transform(movies_df.crew_names)\n",
    "\n",
    "# create a dataframe of the transformed top 100 crew name features\n",
    "crew = pd.DataFrame(t_crew.todense()).add_prefix('crew_')\n",
    "\n",
    "# concatenate these columns into a single dataframe\n",
    "text = pd.concat([overview, title, cast, crew], axis=1)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del overview\n",
    "del title\n",
    "del cast\n",
    "del crew\n",
    "\n",
    "# drop the text columns that have been converted to numeric scores from movies_df\n",
    "movies_df.drop(['overview', 'title', 'cast_names', 'crew_names'], axis=1, inplace=True)\n",
    "\n",
    "# add back to the movies_df the numeric representations of the original text columns\n",
    "movies_df = pd.concat([movies_df.reset_index(), text], axis=1).set_index('imdb_id')\n",
    "print('movies_df shape: ', movies_df.shape)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de1b5e-d963-48d8-8c4c-b1d7dc910b84",
   "metadata": {},
   "source": [
    "### Step 5: Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f67c2be-3cc8-42d8-879f-7d066be6b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rep shape:  (44628, 769)\n"
     ]
    }
   ],
   "source": [
    "# transform movies_df in a numpy array\n",
    "a = movies_df.values\n",
    "\n",
    "# create an instance of a MinMaxScaler and fit it to the numeric data\n",
    "min_max_sc = MinMaxScaler()\n",
    "num_rep = min_max_sc.fit_transform(a)\n",
    "print('num_rep shape: ', num_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3069d4e-e8f6-4547-8bc9-909f9e05a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity of the movies numeric metadata is: 97.4 percent\n"
     ]
    }
   ],
   "source": [
    "# the num_rep dataset has a high degree of sparsity; therefore, we'll use CSR to reduce model runtime (improve space complexity)\n",
    "print('The sparsity of the movies numeric metadata is:',\n",
    "      round((np.size(num_rep)-np.count_nonzero(num_rep))/np.size(num_rep)*100,2),\n",
    "      'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a34490c-f3e5-4285-a731-3a554c67cfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_features_csr:  (44628, 769)\n"
     ]
    }
   ],
   "source": [
    "# reduce the movies dataframe to a csr sparse matrix to reduce classifier run time\n",
    "movie_features_csr = csr_matrix(num_rep)\n",
    "print('movie_features_csr: ', movie_features_csr.shape)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del num_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ceda4-6955-4e8e-87d2-01b95261f899",
   "metadata": {},
   "source": [
    "### Step 6: Reduce dimensionality using PCA and then via feature selection using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026a18d",
   "metadata": {},
   "source": [
    "#### Standardize the data, test the correlation, and reduce using PCA until 70 - 80% explained variance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be24991e-79ac-45e6-beb4-9c8e39646e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a Standard Scaler and fit it to the numeric data\n",
    "sc = StandardScaler()\n",
    "features = sc.fit_transform(a)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f2e1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix between all dataframe values to see if PCA reduction is applicable\n",
    "cm = movies_df.corr().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc886cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 of 769 features (11.96%) have a moderate degree of correlation\n"
     ]
    }
   ],
   "source": [
    "# There are only 92 features with a moderate degree of correlation (~11.96%)\n",
    "# This indicates that PCA reduction might not be the best choice for reducing the number of features\n",
    "mod_corr = np.count_nonzero((cm >= 0.3) & (cm < 1))\n",
    "print(f'{mod_corr} of {cm.shape[1]} features ({\"{:.2%}\".format((mod_corr/cm.shape[1]))}) '\n",
    "      f'have a moderate degree of correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2695068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA on Movie Features... \n",
      "\n",
      "pca_features shape:  (44628, 475)\n",
      "pca_variance:  0.6953204727945989\n"
     ]
    }
   ],
   "source": [
    "# Use PCA to reduce the number of dimensions and check the explained variance sum\n",
    "# According to the literature, a good rule of thumb for fraction of explained variance is 70 - 80%\n",
    "print(\"Running PCA on Movie Features... \\n\")\n",
    "ncomp = 475\n",
    "pca = PCA(n_components=ncomp)\n",
    "pca_features = pca.fit_transform(features)\n",
    "print('pca_features shape: ', pca_features.shape)\n",
    "pca_variance = pca.explained_variance_ratio_.sum()\n",
    "print('pca_variance: {:.3f}'.format(pca_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769f084",
   "metadata": {},
   "source": [
    "#### Find the most important features and filter the dataset via XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a6f97",
   "metadata": {},
   "source": [
    "Since less than 12% of features have a correlation coefficient of >= 0.3 (excluding the correlations between each feature and itself) and a large number of features must be retained in order to explain 70% of the variance, we decided to try another approach to narrow down the number of features in our model. \n",
    "\n",
    "Because it is too computationally expensive to determine which features are most important in determining the rank that each user gives a movie, we chose to narrow the dataset based upon the most important features in determining how well a movie is liked on average. We theorize that most users will like or dislike a film for the same reasons as the general population of reviewers. Since many movies have few reviews, in order to determine which features are important, we will first calculate the Imdb weighted average value for each movie. The formula for the weighted average field was found at the following site: https://medium.com/@developeraritro/building-a-recommendation-system-using-weighted-hybrid-technique-75598b6be8ed\n",
    "\n",
    "Since the Imdb weighted average feature will be continuous and on a different scale than ratings (0 - 5), we will use XGBRegressor to evaluate feature importance. The most important features will then be selected and the hit rate tests for the best performing classifiers re-run to see if reducing the features used improves hit rate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8d4bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the colum for the weighted average\n",
    "v = movies_df['vote_count']\n",
    "R = movies_df['vote_average']\n",
    "C = movies_df['vote_average'].mean()\n",
    "m = movies_df['vote_count'].quantile(.7)\n",
    "movies_df['weighted_avg'] = ((R*v) + (C*m))/(v+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f40baea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data sets from the features\n",
    "# The y variable will be the weighted average rating\n",
    "X, Y = movies_df.loc[:, movies_df.columns != 'weighted_avg'], movies_df.weighted_avg\n",
    "\n",
    "# Split the movies into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eaf34b1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-feedfab9192f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Fit the Grid Search object to the training data from the movies dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[0;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m         )\n\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    195\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1499\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1500\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an instance of an XGBRegressor, with a seed for reproducibility\n",
    "xgb = XGBRegressor(seed = 20)\n",
    "\n",
    "# Define the parameters to be tested with GridSearch CV, using common parameter values\n",
    "params = {'max_depth':[3,6,10], # Set the maximum depth of a tree\n",
    "          'learning_rate':[0.01, 0.05, 0.1], # Set the learning rate\n",
    "          'subsample': [0.6, 0.8, 1], # Set the fraction of data to be used in each fitting step\n",
    "          'colsample_bytree': [0.3, 0.5, 0.7, 1], # Set the fraction of features to be used in each fitting step\n",
    "          }\n",
    "    \n",
    "# Create an instance of a grid search cv object to test combinations of the parameters\n",
    "clf = GridSearchCV(estimator=xgb, \n",
    "                   param_grid=params,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=3)\n",
    "\n",
    "# Fit the Grid Search object to the training data from the movies dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the model with the parameters with the best score on the training data (from GridSearch)\n",
    "optimized_xgb = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature importances in a variable\n",
    "thresholds = optimized_xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the smallest non-zero feature importance\n",
    "min_threshold = min(i for i in thresholds if i > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ddaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of features with an importance greater than or equal to the minimum threshold value\n",
    "np.count_nonzero(optimized_xgb.feature_importances_ >= min_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51427a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features, filtering out only those features with an importance score of 0\n",
    "selection = SelectFromModel(optimized_xgb, threshold=min_threshold, prefit=True)\n",
    "xgb_select_features = selection.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from these features with non-zero importance to be used in our classifier test\n",
    "xgb_features = pd.DataFrame(xgb_select_features)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del xgb_select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048ba08-0c71-48b3-ac53-cdb6f700e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of a MinMaxScaler and fit it to the numeric data\n",
    "min_max_sc = MinMaxScaler()\n",
    "norm_xgb_features = min_max_sc.fit_transform(xgb_features.values)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del xgb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4717a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a CSR sparse matrix\n",
    "xgb_features_csr = csr_matrix(norm_xgb_features)\n",
    "print('xgb_features_csr shape: ', xgb_features_csr.shape)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del norm_xgb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93781965-810d-41a5-a7fb-6f42aef43564",
   "metadata": {},
   "source": [
    "### Step 7: Define functions to test and tune hyperparameters for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d6e79c9-b640-42ec-9838-e26381c688f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf(user_id, clf, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "    \n",
    "    '''Run a classifier for a single user and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extract y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # fit training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # compute probabilities for each class\n",
    "    proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)[:,clf.classes_[clf.classes_==int(hold_out_rating)]].reshape(100,1)\n",
    "    \n",
    "    # store the vote_count for each movie\n",
    "    popularity = movies_data.loc[movies_data.index.isin(test_movies)]['vote_count'].values.reshape(100,1)\n",
    "    \n",
    "    # for each reshuffle\n",
    "    for _ in np.arange(num_reshuffles):\n",
    "        # compute the most popular movie among the top-10 and least popular among the others\n",
    "        most_popular = ranking[(popularity == popularity[ranking > 89].max()) & (ranking > 89)][0]\n",
    "        least_popular = ranking[(popularity == popularity[ranking < 90].min()) & (ranking < 90)][0]\n",
    "        # reshuffle the ranks of the most popular among the top-10 with the least popular among the others\n",
    "        ranking[ranking == most_popular] = 1000\n",
    "        ranking[ranking == least_popular] = -1000\n",
    "        ranking[ranking == 1000] = least_popular\n",
    "        ranking[ranking == -1000] = most_popular\n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    hit = ranking[0] > 89\n",
    "    \n",
    "    # compute tuple (true_label, predicted_label) for hold out test movie\n",
    "    true_label = int(hold_out_rating)\n",
    "    pred_label = clf.predict(X_test)[0]\n",
    "    tup = (pred_label, true_label)\n",
    "    \n",
    "    # compute the novelty score (average number of votes for the top-10 movies)\n",
    "    novelty = popularity[ranking > 89].mean()\n",
    "    \n",
    "    return hit, tup, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521f4068-08ef-4cd2-9314-0a0f805087f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(user_list, clf, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "\n",
    "    '''Compute hit rate, f1 score and novelty score across diferent users'''\n",
    "    \n",
    "    hit_list = []\n",
    "    conf_matrix = {(0,0): 0, (0,1): 0, (1,0): 0, (1,1): 0}\n",
    "    novelty_list = []\n",
    "    for user_id in user_list:\n",
    "        hit, tup, novelty = run_clf(user_id, clf=clf, num_rep=num_rep, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                            movies_data=movies_data, num_reshuffles=num_reshuffles)\n",
    "        hit_list.append(bool(hit))\n",
    "        conf_matrix[tup] += 1\n",
    "        novelty_list.append(novelty)\n",
    "    hit_rate = sum(hit_list) / len(hit_list)\n",
    "    precision = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(1,0)])\n",
    "    recall = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(0,1)])\n",
    "    f1_score = 0 if (precision + recall == 0) else 2 * (precision * recall) / (precision + recall)\n",
    "    novelty_score = sum(novelty_list) / len(novelty_list)\n",
    "    return hit_rate, f1_score, novelty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66d5de2a-329c-424d-a6fe-2fa9479f7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gmm(user_id, n_components, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "    \n",
    "    '''Fit two GMMs, one for the positive and other for the negative examples, and use\n",
    "    the weighted log probabilities to compute the ranking and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :].toarray()\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :].toarray()\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # fit two GMMs, one for the positive labels and one for the negative labels\n",
    "    gmm_0 = GaussianMixture(n_components=n_components, covariance_type='tied', random_state=100)\n",
    "    gmm_0.fit(X_train[y_train==0])\n",
    "    gmm_1 = GaussianMixture(n_components=n_components, covariance_type='tied', random_state=100)\n",
    "    gmm_1.fit(X_train[y_train==1])\n",
    "    \n",
    "    # compute the weighted log probabilities for each test example in both models\n",
    "    log_prob_0 = gmm_0.score_samples(X_test)\n",
    "    log_prob_1 = gmm_1.score_samples(X_test)\n",
    "    proba = np.vstack((log_prob_0, log_prob_1)).T\n",
    "    \n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)[:,int(hold_out_rating)]\n",
    "    \n",
    "    # store the vote_count for each movie\n",
    "    popularity = movies_data.loc[movies_data.index.isin(test_movies)]['vote_count'].values.reshape(100,1)\n",
    "    \n",
    "    # for each reshuffle\n",
    "    for _ in np.arange(num_reshuffles):\n",
    "        # compute the most popular movie among the top-10 and least popular among the others\n",
    "        most_popular = ranking[(popularity == popularity[ranking > 89].max()) & (ranking > 89)][0]\n",
    "        least_popular = ranking[(popularity == popularity[ranking < 90].min()) & (ranking < 90)][0]\n",
    "        # reshuffle the ranks of the most popular among the top-10 with the least popular among the others\n",
    "        ranking[ranking == most_popular] = 1000\n",
    "        ranking[ranking == least_popular] = -1000\n",
    "        ranking[ranking == 1000] = least_popular\n",
    "        ranking[ranking == -1000] = most_popular    \n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    hit = ranking[0] > 89\n",
    "    \n",
    "    # compute tuple (true_label, predicted_label) for hold out test movie\n",
    "    true_label = int(hold_out_rating)\n",
    "    pred_label = 1 if proba[0,1] > proba[0,0] else 0 \n",
    "    tup = (pred_label, true_label)\n",
    "    \n",
    "    # compute the novelty score (average number of votes for the top-10 movies)\n",
    "    novelty = popularity[ranking > 89].mean()\n",
    "    \n",
    "    return hit, tup, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacaff57-b04c-4e07-93c1-4f3e0a52f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_gmm(user_list, n_components, num_rep, train_ratings_data=dev_train, \n",
    "                     test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "\n",
    "    '''Compute hit rate, f1 score and novelty score across diferent users for GMM'''\n",
    "    \n",
    "    hit_list = []\n",
    "    conf_matrix = {(0,0): 0, (0,1): 0, (1,0): 0, (1,1): 0}\n",
    "    novelty_list = []\n",
    "    for user_id in user_list:\n",
    "        hit, tup, novelty = run_gmm(user_id, n_components=n_components, num_rep=num_rep, train_ratings_data=train_ratings_data,\n",
    "                           test_ratings_data=test_ratings_data, movies_data=movies_data, num_reshuffles=num_reshuffles)\n",
    "        hit_list.append(bool(hit))\n",
    "        conf_matrix[tup] += 1\n",
    "        novelty_list.append(novelty)\n",
    "    hit_rate = sum(hit_list) / len(hit_list)\n",
    "    precision = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(1,0)])\n",
    "    recall = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(0,1)])\n",
    "    f1_score = 0 if (precision + recall == 0) else 2 * (precision * recall) / (precision + recall)\n",
    "    novelty_score = sum(novelty_list) / len(novelty_list)\n",
    "    return hit_rate, f1_score, novelty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3fc18d-eba2-4e09-99a4-20ad0ce48a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cosim(user_id, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "    \n",
    "    '''Compute two cosine similarity distances for each example in the test set, one against the positive examples in the training\n",
    "    set and other against the negative examples, and use the distances to compute the ranking and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # compute the cosine similarities, one for the positive labels and one for the negative labels\n",
    "    cosim_0 = cosine_similarity(X_test, X_train[y_train==0]).sum(axis=1)\n",
    "    cosim_1 = cosine_similarity(X_test, X_train[y_train==1]).sum(axis=1)\n",
    "    distance = np.vstack((cosim_0, cosim_1)).T\n",
    "\n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(distance, axis=0)[:,int(hold_out_rating)]\n",
    "    \n",
    "    # store the vote_count for each movie\n",
    "    popularity = movies_data.loc[movies_data.index.isin(test_movies)]['vote_count'].values.reshape(100,1)\n",
    "    \n",
    "    # for each reshuffle\n",
    "    for _ in np.arange(num_reshuffles):\n",
    "        # compute the most popular movie among the top-10 and least popular among the others\n",
    "        most_popular = ranking[(popularity == popularity[ranking < 10].max()) & (ranking < 10)][0]\n",
    "        least_popular = ranking[(popularity == popularity[ranking > 9].min()) & (ranking > 9)][0]\n",
    "        # reshuffle the ranks of the most popular among the top-10 with the least popular among the others\n",
    "        ranking[ranking == most_popular] = 1000\n",
    "        ranking[ranking == least_popular] = -1000\n",
    "        ranking[ranking == 1000] = least_popular\n",
    "        ranking[ranking == -1000] = most_popular\n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in ascending order\n",
    "    hit = ranking[0] < 10\n",
    "    \n",
    "    # compute tuple (true_label, predicted_label) for hold out test movie\n",
    "    true_label = int(hold_out_rating)\n",
    "    pred_label = 1 if distance[0,1] < distance[0,0] else 0 \n",
    "    tup = (pred_label, true_label)\n",
    "    \n",
    "    # compute the novelty score (average number of votes for the top-10 movies)\n",
    "    novelty = popularity[ranking < 10].mean()\n",
    "    \n",
    "    return hit, tup, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a6c68b8-adc3-4a05-960c-dad6088c86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_cosim(user_list, num_rep, train_ratings_data=dev_train, \n",
    "                       test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "\n",
    "    '''Compute hit rate, f1 score and novelty score across diferent users for cosine similarity'''\n",
    "    \n",
    "    hit_list = []\n",
    "    conf_matrix = {(0,0): 0, (0,1): 0, (1,0): 0, (1,1): 0}\n",
    "    novelty_list = []\n",
    "    for user_id in user_list:\n",
    "        hit, tup, novelty = run_cosim(user_id, num_rep=num_rep, train_ratings_data=train_ratings_data,\n",
    "                        test_ratings_data=test_ratings_data, movies_data=movies_data, num_reshuffles=num_reshuffles)\n",
    "        hit_list.append(bool(hit))\n",
    "        conf_matrix[tup] += 1\n",
    "        novelty_list.append(novelty)\n",
    "    hit_rate = sum(hit_list) / len(hit_list)\n",
    "    precision = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(1,0)])\n",
    "    recall = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(0,1)])\n",
    "    f1_score = 0 if (precision + recall == 0) else 2 * (precision * recall) / (precision + recall)\n",
    "    novelty_score = sum(novelty_list) / len(novelty_list)\n",
    "    return hit_rate, f1_score, novelty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f380c2-e388-420f-b0e4-f892d022a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(user_id, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "    \n",
    "    '''Run a random classifier as a baseline'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # assign random probabilities for each class\n",
    "    proba = np.random.uniform(low=0.0, high=1.0, size=200).reshape(100,2)\n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)[:,int(hold_out_rating)]\n",
    "    \n",
    "    # store the vote_count for each movie\n",
    "    popularity = movies_data.loc[movies_data.index.isin(test_movies)]['vote_count'].values.reshape(100,1)\n",
    "    \n",
    "    # for each reshuffle\n",
    "    for _ in np.arange(num_reshuffles):\n",
    "        # compute the most popular movie among the top-10 and least popular among the others\n",
    "        most_popular = ranking[(popularity == popularity[ranking > 89].max()) & (ranking > 89)][0]\n",
    "        least_popular = ranking[(popularity == popularity[ranking < 90].min()) & (ranking < 90)][0]\n",
    "        # reshuffle the ranks of the most popular among the top-10 with the least popular among the others\n",
    "        ranking[ranking == most_popular] = 1000\n",
    "        ranking[ranking == least_popular] = -1000\n",
    "        ranking[ranking == 1000] = least_popular\n",
    "        ranking[ranking == -1000] = most_popular  \n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    hit = ranking[0] > 89\n",
    "    \n",
    "    # compute tuple (true_label, predicted_label) for hold out test movie\n",
    "    true_label = int(hold_out_rating)\n",
    "    pred_label = 1 if proba[0,1] > proba[0,0] else 0 \n",
    "    tup = (pred_label, true_label)\n",
    "    \n",
    "    # compute the novelty score (average number of votes for the top-10 movies)\n",
    "    novelty = popularity[ranking > 89].mean()\n",
    "    \n",
    "    return hit, tup, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56bdfaa4-4f55-4bd4-98ea-8955e5ed50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_baseline(user_list, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "\n",
    "    '''Compute hit rate, f1 score and novelty score across diferent users for random baseline'''\n",
    "    \n",
    "    hit_list = []\n",
    "    conf_matrix = {(0,0): 0, (0,1): 0, (1,0): 0, (1,1): 0}\n",
    "    novelty_list = []\n",
    "    for user_id in user_list:\n",
    "        hit, tup, novelty = run_baseline(user_id, num_rep=num_rep, train_ratings_data=train_ratings_data,\n",
    "                                test_ratings_data=test_ratings_data, movies_data=movies_data, num_reshuffles=num_reshuffles)\n",
    "        hit_list.append(bool(hit))\n",
    "        conf_matrix[tup] += 1\n",
    "        novelty_list.append(novelty)\n",
    "    hit_rate = sum(hit_list) / len(hit_list)\n",
    "    precision = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(1,0)])\n",
    "    recall = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(0,1)])\n",
    "    f1_score = 0 if (precision + recall == 0) else 2 * (precision * recall) / (precision + recall)\n",
    "    novelty_score = sum(novelty_list) / len(novelty_list)\n",
    "    return hit_rate, f1_score, novelty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5c1a766-875f-4231-922a-5924ca466434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(user_id, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "    \n",
    "    '''Run all classifiers for a single user and compute the average probability before returning the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # initialize the models\n",
    "    bnb = BernoulliNB(alpha=1.0)\n",
    "    rfc = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "    svm = SVC(C=1.0, kernel='rbf', probability=True)\n",
    "    knn = KNeighborsClassifier(n_neighbors=15)\n",
    "    lr = LogisticRegression(C=0.1)\n",
    "    gmm_0 = GaussianMixture(n_components=3, covariance_type='tied', random_state=100)\n",
    "    gmm_1 = GaussianMixture(n_components=3, covariance_type='tied', random_state=100)\n",
    "    \n",
    "    # fit training data\n",
    "    bnb.fit(X_train, y_train)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    svm.fit(X_train, y_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "    gmm_0.fit(X_train[y_train==0].toarray())\n",
    "    gmm_1.fit(X_train[y_train==1].toarray())\n",
    "    \n",
    "    # compute probabilities for each model for class==test_ratings\n",
    "    bnb_proba = bnb.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    rfc_proba = rfc.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    svm_proba = svm.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    knn_proba = knn.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    lr_proba = lr.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    gmm_prob_0 = np.exp(gmm_0.score_samples(X_test.toarray()))\n",
    "    gmm_prob_1 = np.exp(gmm_1.score_samples(X_test.toarray()))\n",
    "    gmm_proba = np.vstack((gmm_prob_0, gmm_prob_1)).T[:,int(hold_out_rating)].reshape(100,1)\n",
    "    \n",
    "    # compute the average probabilities\n",
    "    proba = np.hstack((bnb_proba, rfc_proba, svm_proba, knn_proba, lr_proba, gmm_proba)).mean(axis=1)\n",
    "    \n",
    "    # compute the ranking\n",
    "    ranking = np.argsort(proba, axis=0)\n",
    "    \n",
    "    # store the vote_count for each movie\n",
    "    popularity = movies_data.loc[movies_data.index.isin(test_movies)]['vote_count'].values.reshape(100,1)\n",
    "    \n",
    "    # for each reshuffle\n",
    "    for _ in np.arange(num_reshuffles):\n",
    "        # compute the most popular movie among the top-10 and least popular among the others\n",
    "        most_popular = ranking[(popularity == popularity[ranking > 89].max()) & (ranking > 89)][0]\n",
    "        least_popular = ranking[(popularity == popularity[ranking < 90].min()) & (ranking < 90)][0]\n",
    "        # reshuffle the ranks of the most popular among the top-10 with the least popular among the others\n",
    "        ranking[ranking == most_popular] = 1000\n",
    "        ranking[ranking == least_popular] = -1000\n",
    "        ranking[ranking == 1000] = least_popular\n",
    "        ranking[ranking == -1000] = most_popular\n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    hit = ranking[0] > 89\n",
    "    \n",
    "    # compute predictions for hold out test movie from each model\n",
    "    bnb_pred = bnb.predict(X_test)[0]\n",
    "    rfc_pred = rfc.predict(X_test)[0]\n",
    "    svm_pred = svm.predict(X_test)[0]\n",
    "    knn_pred = knn.predict(X_test)[0]\n",
    "    lr_pred = lr.predict(X_test)[0]\n",
    "    gmm_pred = 1 if gmm_prob_1[0] > gmm_prob_0[0] else 0\n",
    "    sum_pred = bnb_pred + rfc_pred + svm_pred + knn_pred + lr_pred + gmm_pred\n",
    "    \n",
    "    # compute tuple (true_label, predicted_label) for hold out test movie\n",
    "    true_label = int(hold_out_rating)\n",
    "    pred_label = 1 if sum_pred >= 3 else 0 \n",
    "    tup = (pred_label, true_label)\n",
    "    \n",
    "    # compute the novelty score (average number of votes for the top-10 movies)\n",
    "    novelty = popularity[ranking > 89].mean()\n",
    "    \n",
    "    return hit, tup, novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef246225-2c66-4554-841f-e08c96cd2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_ensemble(user_list, num_rep, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_reshuffles=0):\n",
    "\n",
    "    '''Compute hit rate, f1 score and novelty score across diferent users for ensemble method'''\n",
    "    \n",
    "    hit_list = []\n",
    "    conf_matrix = {(0,0): 0, (0,1): 0, (1,0): 0, (1,1): 0}\n",
    "    novelty_list = []\n",
    "    for user_id in user_list:\n",
    "        hit, tup, novelty = run_ensemble(user_id, num_rep=num_rep, train_ratings_data=train_ratings_data,\n",
    "                                test_ratings_data=test_ratings_data, movies_data=movies_data, num_reshuffles=num_reshuffles)\n",
    "        hit_list.append(bool(hit))\n",
    "        conf_matrix[tup] += 1\n",
    "        novelty_list.append(novelty)\n",
    "    hit_rate = sum(hit_list) / len(hit_list)\n",
    "    precision = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(1,0)])\n",
    "    recall = conf_matrix[(1,1)] / (conf_matrix[(1,1)] + conf_matrix[(0,1)])\n",
    "    f1_score = 0 if (precision + recall == 0) else 2 * (precision * recall) / (precision + recall)\n",
    "    novelty_score = sum(novelty_list) / len(novelty_list)\n",
    "    return hit_rate, f1_score, novelty_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e6bfd-b520-4184-9e3d-458599f35836",
   "metadata": {},
   "source": [
    "### Step 8: Test different classifiers using the unreduced movies features converted to a CSR sparse matrix\n",
    "\n",
    "In this section, we test the hit rates and f1 scores obtained via various classifiers that we've learned this semester, and compare the rates to that obtained by a random classifier. In each classifier test, all movie features are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "831fe004-7ab7-426e-b01c-e89d04054742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "clf_list = ['Base', 'Cosine Sim', 'BNB', 'RF', 'SVM', 'KNN', 'LR', 'GMM', 'Ensemble']\n",
    "clf_hit_rate_scores = []\n",
    "clf_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94c74b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random user list of [sample_size] users from the dev set to be tested via all classifiers\n",
    "np.random.seed(100)\n",
    "user_list = np.random.choice(dev_train['userId'].unique(), size=sample_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df8719-7cbe-413f-912b-ff0b6281a392",
   "metadata": {},
   "source": [
    "#### Baseline (random classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00672b7-9888-434d-8c05-c31b33f7f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (random classifier)\n",
      "hit rate==0.091 f1 score==0.511\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate and f1 score\n",
    "hit_rate, f1_score, novelty_score = get_hit_rate_baseline(user_list, num_rep=movie_features_csr)\n",
    "# print results\n",
    "print('Baseline (random classifier)')\n",
    "print('hit rate=={:.3f} f1 score=={:.3f}'.format(hit_rate, f1_score))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(hit_rate)\n",
    "clf_f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae1fc6-a6f7-4c0a-ac52-1994f8786016",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14e05fc2-3371-443a-8bdc-aeb0d57b427b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity\n",
      "hit rate==0.089 f1 score==0.362\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate and f1 score\n",
    "hit_rate, f1_score, novelty_score = get_hit_rate_cosim(user_list, num_rep=movie_features_csr)\n",
    "# print results\n",
    "print('Cosine similarity')\n",
    "print('hit rate=={:.3f} f1 score=={:.3f}'.format(hit_rate, f1_score))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(hit_rate)\n",
    "clf_f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5baf416-e9b8-4b69-9e6d-5bc580c5d8b4",
   "metadata": {},
   "source": [
    "#### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7f56743-2da4-4886-b244-8427115f12bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB\n",
      "For alpha==0.00001 hit rate==0.103 f1 score==0.639\n",
      "For alpha==0.00010 hit rate==0.109 f1 score==0.639\n",
      "For alpha==0.00100 hit rate==0.109 f1 score==0.637\n",
      "For alpha==0.01000 hit rate==0.103 f1 score==0.634\n",
      "For alpha==0.10000 hit rate==0.092 f1 score==0.640\n",
      "For alpha==1.00000 hit rate==0.113 f1 score==0.661\n",
      "--------------------------------------------------\n",
      "Best param: 1.00000\n",
      "Best f1 score: 0.661\n"
     ]
    }
   ],
   "source": [
    "# define range for alpha parameter\n",
    "param_range = [0.001, 0.01, 0.1, 1, 10]\n",
    "# for different values of parameter alpha\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('BernoulliNB')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    bnb = BernoulliNB(alpha=param)\n",
    "    # compute the hit rate and f1 score\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate(user_list, num_rep=movie_features_csr, clf=bnb)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For alpha=={:.3f} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_bnb_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.3f}'.format(best_bnb_param))\n",
    "print('Best f1 score: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad0e34-b6e3-496b-b5a7-ac1146226a98",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeb2f3bc-7357-4d1a-9581-774eec0fc6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "For n_estimators==  1 hit rate==0.105 f1 score==0.620\n",
      "For n_estimators==  2 hit rate==0.129 f1 score==0.501\n",
      "For n_estimators==  5 hit rate==0.129 f1 score==0.653\n",
      "For n_estimators== 10 hit rate==0.121 f1 score==0.658\n",
      "For n_estimators== 50 hit rate==0.111 f1 score==0.680\n",
      "For n_estimators==100 hit rate==0.101 f1 score==0.684\n",
      "--------------------------------------------------\n",
      "Best param: 100\n",
      "Best f1 score: 0.684\n"
     ]
    }
   ],
   "source": [
    "# define range for n_estimators parameter\n",
    "param_range = [5, 10, 50, 100, 500]\n",
    "# for different values of n_estimators parameter\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('Random Forest')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    rfc = RandomForestClassifier(n_estimators=param, criterion='entropy')\n",
    "    # compute the hit rate\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate(user_list, num_rep=movie_features_csr, clf=rfc)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For n_estimators=={:3} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_rf_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:3}'.format(best_rf_param))\n",
    "print('Best f1 score: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a5676-8af8-4895-ae38-0326aed15db9",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "898af55b-b539-4793-9a1c-659981c071c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "For C==0.001 hit rate==0.097 f1 score==0.656\n",
      "For C==0.010 hit rate==0.097 f1 score==0.656\n",
      "For C==0.100 hit rate==0.109 f1 score==0.656\n",
      "For C==1.000 hit rate==0.088 f1 score==0.684\n",
      "For C==10.000 hit rate==0.100 f1 score==0.668\n",
      "--------------------------------------------------\n",
      "Best param: 1.000\n",
      "Best f1 score: 0.684\n"
     ]
    }
   ],
   "source": [
    "# define range for C parameter\n",
    "param_range = [0.001, 0.01, 0.1, 1, 10]\n",
    "# for different values of C parameter\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('SVM')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    svm = SVC(C=param, kernel='rbf', probability=True)\n",
    "    # compute the hit rate\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate(user_list, num_rep=movie_features_csr, clf=svm)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For C=={:.3f} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_svm_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.3f}'.format(best_svm_param))\n",
    "print('Best f1 score: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079dc5de-ee05-4a58-9d54-fa578fb82cd8",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c39cd46b-d104-408f-9298-7a9c5371b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "For k== 5 hit rate==0.152 f1 score==0.651\n",
      "For k==10 hit rate==0.104 f1 score==0.627\n",
      "For k==15 hit rate==0.104 f1 score==0.667\n",
      "For k==20 hit rate==0.106 f1 score==0.642\n",
      "For k==25 hit rate==0.134 f1 score==0.663\n",
      "--------------------------------------------------\n",
      "Best param: 15\n",
      "Best hit rate: 0.667\n"
     ]
    }
   ],
   "source": [
    "# define range for k (n_neighbors) parameter\n",
    "param_range = [5, 10, 15, 20, 25]\n",
    "# for different values of parameter k (n_beighbors)\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('K-Nearest Neighbors')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=param)\n",
    "    # compute the hit rate\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate(user_list, num_rep=movie_features_csr, clf=knn)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For k=={:2} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_knn_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:2}'.format(best_knn_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1567233-0683-41e3-9902-4ad033b9d84b",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5ebaa78-36d0-4751-b15b-a2be40013ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "For C==0.00001 hit rate==0.091 f1 score==0.655\n",
      "For C==0.00010 hit rate==0.097 f1 score==0.655\n",
      "For C==0.00100 hit rate==0.098 f1 score==0.656\n",
      "For C==0.01000 hit rate==0.095 f1 score==0.659\n",
      "For C==0.10000 hit rate==0.086 f1 score==0.679\n",
      "For C==1.00000 hit rate==0.089 f1 score==0.678\n",
      "--------------------------------------------------\n",
      "Best param: 0.10000\n",
      "Best hit rate: 0.679\n"
     ]
    }
   ],
   "source": [
    "# define range for C parameter\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "# for different values of C parameter\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('Logistic Regression')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    lr = LogisticRegression(C=param)\n",
    "    # compute the hit rate\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate(user_list, num_rep=movie_features_csr, clf=lr)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For C=={:.4f} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_lr_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.4f}'.format(best_lr_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b8c3a-1a80-45b2-8651-2a9f223a8bef",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d5593b6-146a-49a1-b0fe-4dc6b2d25133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM\n",
      "For n_components==1 hit rate==0.112 f1 score==0.667\n",
      "For n_components==2 hit rate==0.100 f1 score==0.668\n",
      "For n_components==3 hit rate==0.090 f1 score==0.671\n",
      "For n_components==4 hit rate==0.099 f1 score==0.662\n",
      "For n_components==5 hit rate==0.108 f1 score==0.669\n",
      "--------------------------------------------------\n",
      "Best param: 3\n",
      "Best hit rate: 0.671\n"
     ]
    }
   ],
   "source": [
    "# define range for n_components parameter\n",
    "param_range = [1, 2, 3, 4, 5]\n",
    "# for different values of n_components parameter\n",
    "hit_rate_list = []\n",
    "f1_score_list = []\n",
    "print('GMM')\n",
    "for param in param_range:\n",
    "    # compute the hit rate and f1 score\n",
    "    hit_rate, f1_score, novelty_score = get_hit_rate_gmm(user_list, num_rep=movie_features_csr, n_components=param)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # append f1 score to the f1_score_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    # print results\n",
    "    print('For n_components=={} hit rate=={:.3f} f1 score=={:.3f}'.format(param, hit_rate, f1_score))\n",
    "best_gmm_param = param_range[f1_score_list.index(max(f1_score_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {}'.format(best_gmm_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(f1_score_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b2400-dfb9-4515-a35e-c4c4ca280222",
   "metadata": {},
   "source": [
    "#### Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfc5cf69-f277-4758-8f98-e2fd613fd7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Learning\n",
      "hit rate==0.097, f1 score==0.683\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate\n",
    "hit_rate, f1_score, novelty_score = get_hit_rate_ensemble(user_list, num_rep=movie_features_csr)\n",
    "# print results\n",
    "print('Ensemble Learning')\n",
    "print('hit rate=={:.3f}, f1 score=={:.3f}'.format(hit_rate, f1_score))\n",
    "# append best_score to clf_scores\n",
    "clf_hit_rate_scores.append(max(hit_rate_list))\n",
    "clf_f1_scores.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f674f-7097-421c-bde9-56b0ce1939b8",
   "metadata": {},
   "source": [
    "#### Plot comparative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9638bec3-02eb-418a-8044-6804764cf7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.bar(clf_list[1:], clf_hit_rate_scores[1:], alpha=0.5)\n",
    "ax.hlines(clf_hit_rate_scores[0], xmin=-0.5, xmax=7.5, color='black', linestyles='dashed', label='baseline')\n",
    "ax.text(7.6, clf_hit_rate_scores[0], 'baseline')\n",
    "ax.set_ylabel('Hit Ratio')\n",
    "ax.set_ylim(0.0, 0.2)\n",
    "ax.set_title('Best Hit Ratio by Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27ce254c-9106-4672-9454-7212bdae44d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.bar(clf_list[1:], clf_f1_scores[1:], alpha=0.5)\n",
    "ax.hlines(clf_f1_scores[0], xmin=-0.5, xmax=7.5, color='black', linestyles='dashed', label='baseline')\n",
    "ax.text(7.6, clf_f1_scores[0], 'baseline')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_ylim(0.0, 0.8)\n",
    "ax.set_title('Best F1 Score by Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5c8e2",
   "metadata": {},
   "source": [
    "### Step 9: Rerun the best models for several of the higher performing classifiers, but do so using the reduced parameters obtained by PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683dc49",
   "metadata": {},
   "source": [
    "In this section, we'll use the same user list randomly generated above to test the classifiers, using the PCA reduced set of features in place of movies_df. To test the classifiers, we assigned the classifier the parameter with the best hit rate as found on the unreduced data (in theory, the classifiers should perform better on reduced data, due to decreased overfitting). The findings appear to confirm our previous theory that PCA dimensionality reduction is not a good fit - hit rate scores are lower than on unreduced data, even for the highest performing classifers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b3e08",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "70ea5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==100 hit rate on pca-reduced features==0.117 and f1 score==0.675\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "rfc = RandomForestClassifier(n_estimators=best_rf_param, criterion='entropy')\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate, pca_f1_score, pca_novelty_score = get_hit_rate(user_list, clf=rfc, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={} on pca-reduced features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_rf_param, pca_hit_rate, pca_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a43d0f",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61f96499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C==  1 hit rate on pca-reduced features==0.092 and f1 score==0.669\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "svm = SVC(C=best_svm_param, kernel='rbf', probability=True)\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate, pca_f1_score, pca_novelty_score = get_hit_rate(user_list, clf=rfc, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For C=={} on pca-reduced features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_svm_param, pca_hit_rate, pca_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bb7b7",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8096706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C==0.1 hit rate on pca-reduced features==0.093 and f1 score==0.652\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "lr = LogisticRegression(C=best_lr_param)\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate, pca_f1_score, pca_novelty_score = get_hit_rate(user_list, clf=lr, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For C=={} on pca-reduced features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_lr_param, pca_hit_rate, pca_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d4a91",
   "metadata": {},
   "source": [
    "### Step 10: Rerun the best models for several of the higher performing classifiers, but do so using the reduced fetures obtained from feature importance values from a hyperparameter-tuned XGBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57e562",
   "metadata": {},
   "source": [
    "#### Test various classifiers on the dataset of reduced movie features obtained by feature importance values from a hyperparameter-tuned XGBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3f40",
   "metadata": {},
   "source": [
    "Similarly to our findings when testing the classifiers on PCA-reduced features, we also found worse performance when evaluating our models on a dataset with reduced features (those with non-zero importance to the weighted average, selected by XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d0bb7",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "rfc = RandomForestClassifier(n_estimators=best_rf_param, criterion='entropy')\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate, select_features_f1_score, select_features_novelty_score = get_hit_rate(user_list, clf=rfc, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} on selected features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_rf_param, select_features_hit_rate, \n",
    "                                                                                                 select_features_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aed2ee",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0143c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "svm = SVC(C=best_svm_param, kernel='rbf', probability=True)\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate, select_features_f1_score, select_features_novelty_score = get_hit_rate(user_list, clf=rfc, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For C=={:3} on selected features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_svm_param, select_features_hit_rate, \n",
    "                                                                                      select_features_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda214",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "lr = LogisticRegression(C=best_lr_param)\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate, select_features_f1_score, select_features_novelty_score = get_hit_rate(user_list, clf=lr, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For C=={:3} on selected features hit rate=={:.3f} and f1 score=={:.3f}'.format(best_lr_param, select_features_hit_rate, \n",
    "                                                                                      select_features_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c868f4e-62ba-4edb-86c3-526db26535cf",
   "metadata": {},
   "source": [
    "### Step 11: Evaluate trade-off between hit rate and novelty score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f136aae-5174-4e62-aeea-ad0be8b1a591",
   "metadata": {},
   "source": [
    "For this section, we will use only the best classifiers (xxx, yyy, zzz) trained on xxx data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
