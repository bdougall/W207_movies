{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9533b06-7110-418e-9472-fc5b5117f953",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8394217b-7ec5-47b9-92ab-fc212d6c00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7dd9b-c232-4dc4-bba3-f3b3d4576d55",
   "metadata": {},
   "source": [
    "### Step 2: Define working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab05abd0-103d-4964-96ab-b799df715b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data = 'C:/Users/britt/Desktop/W207/final_project/raw_data/'\n",
    "path_clean_data = 'C:/Users/britt/Desktop/W207/final_project/clean_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826754a3-e66a-490a-9394-c965033b75a2",
   "metadata": {},
   "source": [
    "### Step 3: Load clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d2d-65bd-4909-8025-b53a7a472b0b",
   "metadata": {},
   "source": [
    "#### Split ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e3572a-528c-4b60-8958-770842618794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ratings dataframes\n",
    "dev_train = pd.read_csv(path_clean_data + 'dev_train.csv')\n",
    "dev_test = pd.read_csv(path_clean_data + 'dev_test.csv')\n",
    "test_train = pd.read_csv(path_clean_data + 'test_train.csv')\n",
    "test_test = pd.read_csv(path_clean_data + 'test_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ed7d75-a2e1-414a-a2aa-5b5e6b453a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'Unnamed 0' column\n",
    "dev_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dev_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_test.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ecbdef-1d61-4601-8c73-0f34ca00b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "dev_train: (889501, 3)\n",
      "dev_test: (5000, 3)\n",
      "test_train: (19563173, 3)\n",
      "test_test: (110715, 3)\n"
     ]
    }
   ],
   "source": [
    "# print dataframes shapes\n",
    "print('Shapes')\n",
    "print('dev_train:', dev_train.shape)\n",
    "print('dev_test:', dev_test.shape)\n",
    "print('test_train:', test_train.shape)\n",
    "print('test_test:', test_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0a4767-136b-4c42-9546-9eccfd51fe89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n",
      "Index(['userId', 'imdb_id', 'rating'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print dataframes columns\n",
    "print(dev_train.columns)\n",
    "print(dev_test.columns)\n",
    "print(test_train.columns)\n",
    "print(test_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6738b2-7106-4798-88ae-ebbc4764507b",
   "metadata": {},
   "source": [
    "#### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33937f1e-11ba-44b4-815c-348797407cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load movies dataframe\n",
    "movies_df = pd.read_csv(path_clean_data + 'movies_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b2fc61-5d5b-4357-bf17-24dd9bd8bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated imdb_id:  3\n"
     ]
    }
   ],
   "source": [
    "# print remaining imdb_id duplicates (if any)\n",
    "print('Number of duplicated imdb_id: ', movies_df[movies_df['imdb_id'].duplicated()].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42bed652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining imdb_id duplicates, keeping the last indexed entry\n",
    "movies_df = movies_df[~movies_df['imdb_id'].duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92f1af8-da50-45bb-b958-cdb87cb47634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set imdb_id as index\n",
    "movies_df.set_index('imdb_id', verify_integrity=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5981321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5505064085327597\n",
      "0.021309491798870664\n",
      "0.31612440620238413\n",
      "0.27189208568611634\n",
      "0.06085865375997132\n"
     ]
    }
   ],
   "source": [
    "# Find the proportion of movies with an unknown tagline\n",
    "# Since more than half of movies in the dataset don't have a tagline, we'll drop this field\n",
    "print(len(movies_df[movies_df.tagline == 'unknown'])/(len(movies_df)))\n",
    "\n",
    "# Find the proportion of movies with unknown overview - only about 2% of movies are missing this field\n",
    "print(len(movies_df[movies_df.overview == 'unknown'])/(len(movies_df)))\n",
    "\n",
    "# Find the proportion of movies with unknown description - ~ one-third of movies don't have a result for this field\n",
    "# Since the overview field also contains a description, but contains fewer missing values, we'll use this field\n",
    "print(len(movies_df[movies_df.description == 'unknown'])/(len(movies_df)))\n",
    "\n",
    "# Find the proportion of movies without production companies or production countries listed\n",
    "# Nearly a third of films in the dataset don't have a value for production companies or production countries\n",
    "# We'll drop these fields\n",
    "print(len(\n",
    "    (movies_df[(movies_df.production_companies == 'unknown') | (movies_df.production_countries == 'unknown')]))\n",
    "                                                     /len(movies_df))\n",
    "\n",
    "# Only a small proportion of films are missing cast or crew names, so we'll keep these fields\n",
    "print(len(\n",
    "    (movies_df[(movies_df.cast_names == 'unknown') | (movies_df.crew_names == 'unknown')]))\n",
    "                                                     /len(movies_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44445309-34b5-4d5f-8426-07c260c3ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove fields unnamed: 0, id, tagline, description, production_countries, production_companies\n",
    "movies_df = movies_df.drop(['Unnamed: 0', 'id', 'tagline', \n",
    "                            'description', 'production_countries', 'production_companies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44fbcf4b-a9ed-4a4e-9fdc-ca6cd56cdd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44628, 173)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe shape\n",
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e287c09-ab07-476e-b8c1-541cb3fd5899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adult', 'belongs_to_collection', 'budget', 'originally_english',\n",
       "       'overview', 'popularity', 'revenue', 'runtime', 'title', 'video',\n",
       "       ...\n",
       "       'zh', 'zu', 'canceled', 'in-production', 'planned', 'post-production',\n",
       "       'released', 'rumored', 'cast_names', 'crew_names'],\n",
       "      dtype='object', length=173)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe columns\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d973a-0c48-4a18-b8cc-bc6777cd297f",
   "metadata": {},
   "source": [
    "### Step 4: Vectorize text fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbb556",
   "metadata": {},
   "source": [
    "Since we had insufficient memory to keep all text features from all fields, we chose to limit the features retained from transforming the text features in the literature to a subset of features. Based upon examples in the literature, we chose to keep the top 200 most significant words to the overview and title fields. We elected to further narrow the crew and cast names to the 100 most significant crew and cast names respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4008eb35-3136-45e9-8914-805f89539194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_df shape:  (44628, 769)\n"
     ]
    }
   ],
   "source": [
    "# create an instance of a TfidfVectorizer object for overview\n",
    "# set it to keep the top 200 most significant words in the overview column\n",
    "tfidf_overview = TfidfVectorizer(max_features=200)\n",
    "t_overview = tfidf_overview.fit_transform(movies_df.overview)\n",
    "\n",
    "# create a dataframe of the transformed top 200 overview features\n",
    "overview = pd.DataFrame(t_overview.todense()).add_prefix('overview_')\n",
    "\n",
    "# fill the NA values in the title column with unknown\n",
    "movies_df.title.fillna('unknown', inplace=True)\n",
    "\n",
    "# create an instance of a TfidfVectorizer object for title\n",
    "# set it to keep the top 200 most significant words in the title column\n",
    "tfidf_title = TfidfVectorizer(max_features=200)\n",
    "t_title = tfidf_title.fit_transform(movies_df.title)\n",
    "\n",
    "# create a dataframe of the transformed top 200 overview features\n",
    "title = pd.DataFrame(t_title.todense()).add_prefix('title_')\n",
    "\n",
    "# create an instance of a TfidfVectorizer object for cast names, keeping the 100 most significant cast names\n",
    "tfidf_cast = TfidfVectorizer(max_features=100)\n",
    "t_cast = tfidf_cast.fit_transform(movies_df.cast_names)\n",
    "\n",
    "# create a dataframe of the transformed top 100 cast name features\n",
    "cast = pd.DataFrame(t_cast.todense()).add_prefix('cast_')\n",
    "\n",
    "# create an instance of a TfidfVectorizer for crew names, keeping the 100 most significant crew names\n",
    "tfidf_crew = TfidfVectorizer(max_features=100)\n",
    "t_crew = tfidf_crew.fit_transform(movies_df.crew_names)\n",
    "\n",
    "# create a dataframe of the transformed top 100 crew name features\n",
    "crew = pd.DataFrame(t_crew.todense()).add_prefix('crew_')\n",
    "\n",
    "# concatenate these columns into a single dataframe\n",
    "text = pd.concat([overview, title, cast, crew], axis=1)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del overview\n",
    "del title\n",
    "del cast\n",
    "del crew\n",
    "\n",
    "# drop the text columns that have been converted to numeric scores from movies_df\n",
    "movies_df.drop(['overview', 'title', 'cast_names', 'crew_names'], axis=1, inplace=True)\n",
    "\n",
    "# add back to the movies_df the numeric representations of the original text columns\n",
    "movies_df = pd.concat([movies_df.reset_index(), text], axis=1).set_index('imdb_id')\n",
    "print('movies_df shape: ', movies_df.shape)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6863c28-da46-45d2-a67f-437390bb4b0c",
   "metadata": {},
   "source": [
    "### Step 5: Normalize features (all between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eed62bf-ea38-4e1b-84e2-8df87a31d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rep shape:  (44628, 769)\n"
     ]
    }
   ],
   "source": [
    "# transform movies_df in a numpy array\n",
    "a = movies_df.iloc[:,0:].values\n",
    "\n",
    "# create an instance of a MinMaxScaler and fit it to the numeric data\n",
    "min_max_sc = MinMaxScaler()\n",
    "num_rep = min_max_sc.fit_transform(a)\n",
    "print('num_rep shape: ', num_rep.shape)\n",
    "\n",
    "# free-up memory deleting intermediate objects\n",
    "del a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ceda4-6955-4e8e-87d2-01b95261f899",
   "metadata": {},
   "source": [
    "### Step 6: Reduce dimensionality using PCA and then via feature selection using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026a18d",
   "metadata": {},
   "source": [
    "#### Standardize the data, test the correlation, and reduce using PCA until 70 - 80% explained variance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be24991e-79ac-45e6-beb4-9c8e39646e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of the values in the dataframe\n",
    "x = movies_df.iloc[:,0:].values\n",
    "\n",
    "# Create an instance of a Standard Scaler and fit it to the numeric data\n",
    "sc = StandardScaler()\n",
    "features = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f2e1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix between all dataframe values to see if PCA reduction is applicable\n",
    "cm = movies_df.corr().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc886cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 of 769 features (11.96%) have a moderate degree of correlation\n"
     ]
    }
   ],
   "source": [
    "# There are only 92 features with a moderate degree of correlation (~11.96%)\n",
    "# This indicates that PCA reduction might not be the best choice for reducing the number of features\n",
    "mod_corr = np.count_nonzero((cm >= 0.3) & (cm < 1))\n",
    "print(f'{mod_corr} of {cm.shape[1]} features ({\"{:.2%}\".format((mod_corr/cm.shape[1]))}) '\n",
    "      f'have a moderate degree of correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2695068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA on Movie Features... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6957470980869969"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use PCA to reduce the number of dimensions and check the explained variance sum\n",
    "# According to the literature, a good rule of thumb for fraction of explained variance is 70 - 80%\n",
    "print(\"Running PCA on Movie Features... \\n\")\n",
    "ncomp = 475\n",
    "pca = PCA(n_components=ncomp)\n",
    "pca_features = pca.fit_transform(features) \n",
    "pca_variance = pca.explained_variance_ratio_.sum()\n",
    "pca_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769f084",
   "metadata": {},
   "source": [
    "#### Find the most important features and filter the dataset via XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a6f97",
   "metadata": {},
   "source": [
    "Since less than 12% of features have a correlation coefficient of >= 0.3 (excluding the correlations between each feature and itself) and a large number of features must be retained in order to explain 70% of the variance, we decided to try another approach to narrow down the number of features in our model. \n",
    "\n",
    "Because it is too computationally expensive to determine which features are most important in determining the rank that each user gives a movie, we chose to narrow the dataset based upon the most important features in determining how well a movie is liked on average. We theorize that most users will like or dislike a film for the same reasons as the general population of reviewers. Since many movies have few reviews, in order to determine which features are important, we will first calculate the Imdb weighted average value for each movie. The formula for the weighted average field was found at the following site: https://medium.com/@developeraritro/building-a-recommendation-system-using-weighted-hybrid-technique-75598b6be8ed\n",
    "\n",
    "Since the Imdb weighted average feature will be continuous and on a different scale than ratings (0 - 5), we will use XGBRegressor to evaluate feature importance. The most important features will then be selected and the hit rate tests for the best performing classifiers re-run to see if reducing the features used improves hit rate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d4bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the colum for the weighted average\n",
    "v = movies_df['vote_count']\n",
    "R = movies_df['vote_average']\n",
    "C = movies_df['vote_average'].mean()\n",
    "m = movies_df['vote_count'].quantile(.7)\n",
    "movies_df['weighted_avg'] = ((R*v) + (C*m))/(v+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f40baea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data sets from the features\n",
    "# The y variable will be the weighted average rating\n",
    "X, Y = movies_df.loc[:, movies_df.columns != 'weighted_avg'], movies_df.weighted_avg\n",
    "\n",
    "# Split the movies into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf34b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None, gamma=None,\n",
       "                                    gpu_id=None, importance_type='gain',\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_weight=None,\n",
       "                                    missing=nan, monotone_constraints=None,\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    num_parallel_tree=None, random_state=None,\n",
       "                                    reg_alpha=None, reg_lambda=None,\n",
       "                                    scale_pos_weight=None, seed=20,\n",
       "                                    subsample=None, tree_method=None,\n",
       "                                    validate_parameters=None, verbosity=None),\n",
       "             param_grid={'colsample_bytree': [0.3, 0.5, 0.7, 1],\n",
       "                         'learning_rate': [0.01, 0.05, 0.1],\n",
       "                         'max_depth': [3, 6, 10], 'subsample': [0.6, 0.8, 1]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of an XGBRegressor, with a seed for reproducibility\n",
    "xgb = XGBRegressor(seed = 20)\n",
    "\n",
    "# Define the parameters to be tested with GridSearch CV, using common parameter values\n",
    "params = {'max_depth':[3,6,10], # Set the maximum depth of a tree\n",
    "          'learning_rate':[0.01, 0.05, 0.1], # Set the learning rate\n",
    "          'subsample': [0.6, 0.8, 1], # Set the fraction of data to be used in each fitting step\n",
    "          'colsample_bytree': [0.3, 0.5, 0.7, 1], # Set the fraction of features to be used in each fitting step\n",
    "          }\n",
    "    \n",
    "# Create an instance of a grid search cv object to test combinations of the parameters\n",
    "clf = GridSearchCV(estimator=xgb, \n",
    "                   param_grid=params,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=3)\n",
    "\n",
    "# Fit the Grid Search object to the training data from the movies dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a85b120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the model with the parameters with the best score on the training data (from GridSearch)\n",
    "optimized_xgb = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc4f055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature importances in a variable\n",
    "thresholds = optimized_xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24fd1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the smallest non-zero feature importance\n",
    "min_threshold = min(i for i in thresholds if i > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "199ddaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of features with an importance greater than or equal to the minimum threshold value\n",
    "np.count_nonzero(optimized_xgb.feature_importances_ >= min_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51427a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features, filtering out only those features with an importance score of 0\n",
    "selection = SelectFromModel(optimized_xgb, threshold=min_threshold, prefit=True)\n",
    "xgb_select_features = selection.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "262f2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from these features with non-zero importance to be used in our classifier test\n",
    "xgb_features = pd.DataFrame(xgb_select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc4717a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a CSR sparse matrix\n",
    "xgb_features_csr = csr_matrix(xgb_features.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93781965-810d-41a5-a7fb-6f42aef43564",
   "metadata": {},
   "source": [
    "### Step 7: Test and tune hyperparameters for different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2e761-89a1-4003-bd2d-05d2567eaeb4",
   "metadata": {},
   "source": [
    "#### Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d6e79c9-b640-42ec-9838-e26381c688f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf(user_id, clf, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Run a classifier for a single user and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extract y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # fit training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # compute probabilities for each class\n",
    "    proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)[:,clf.classes_[clf.classes_==int(hold_out_rating)]]\n",
    "    \n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    score = ranking[0] > 89\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "521f4068-08ef-4cd2-9314-0a0f805087f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(user_list, clf, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "\n",
    "    '''Compute hit rate across diferent users'''\n",
    "    \n",
    "    hit_list = []\n",
    "    for user_id in user_list:\n",
    "        score = run_clf(user_id, clf=clf, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                            movies_data=movies_data, num_rep=num_rep)\n",
    "        hit_list.append(bool(score))\n",
    "    return sum(hit_list) / len(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a350eb0c-ac9d-48e2-86a2-45ffd856618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_by_rating(user_list, clf, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Compute hit rate by user rating in the hold out movie'''\n",
    "    \n",
    "    # for different ratings in the hold out movie\n",
    "    ratings = [0, 1]\n",
    "    hit_rate_by_rating = []\n",
    "    for r in ratings:\n",
    "        # define the user_list as the subset of users who gave rate r in the hold out movie\n",
    "        user_list = test_ratings_data[test_ratings_data['rating']==r]['userId']\n",
    "        # compute the hit rate\n",
    "        hit_rate = get_hit_rate(user_list=user_list, clf=clf, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                                movies_data=movies_df, num_rep=num_rep)\n",
    "        # append hit rate to the hit_rate_by_rating\n",
    "        hit_rate_by_rating.append(hit_rate)\n",
    "        # print results\n",
    "        print('For rating=={} hit rate=={:.3f}'.format(r, hit_rate))\n",
    "    \n",
    "    return hit_rate_by_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3923ad4-422f-4461-973f-2e3c351ceed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_by_n_ratings(user_list, clf, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Compute hit rate by number of ratings in the user training set'''\n",
    "    \n",
    "    # split users by number of ratings\n",
    "    bins = [0, 50, 100, 150, 200, 20000]\n",
    "    user_list_by_n_ratings = pd.cut(train_ratings_data.groupby('userId').count()['rating'], bins).reset_index('userId')\n",
    "    intervals = user_list_by_n_ratings['rating'].unique()\n",
    "    # for different intervals of number of ratings\n",
    "    hit_rate_by_n_ratings = []\n",
    "    for i in intervals:\n",
    "        # define the user_list as the subset of users within the interval\n",
    "        user_list = user_list_by_n_ratings[user_list_by_n_ratings['rating']==i]['userId']\n",
    "        # compute the hit rate\n",
    "        hit_rate = get_hit_rate(user_list=user_list, clf=clf, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                                movies_data=movies_df, num_rep=num_rep)\n",
    "        # append hit rate to the hit_rate_by_n_ratings\n",
    "        hit_rate_by_n_ratings.append(hit_rate)\n",
    "        # print results\n",
    "        print('For interval=={} hit rate=={:.3f}'.format(i, hit_rate))\n",
    "        \n",
    "    return hit_rate_by_n_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66d5de2a-329c-424d-a6fe-2fa9479f7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gmm(user_id, n_components, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Fit two GMMs, one for the positive and other for the negative examples, and use\n",
    "    the weighted log probabilities to compute the ranking and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # fit two GMMs, one for the positive labels and one for the negative labels\n",
    "    gmm_0 = GaussianMixture(n_components=n_components, covariance_type='full', random_state=100)\n",
    "    gmm_0.fit(X_train[y_train==0])\n",
    "    gmm_1 = GaussianMixture(n_components=n_components, covariance_type='full', random_state=100)\n",
    "    gmm_1.fit(X_train[y_train==1])\n",
    "    \n",
    "    # compute the weighted log probabilities for each test example in both models\n",
    "    log_prob_0 = gmm_0.score_samples(X_test)\n",
    "    log_prob_1 = gmm_1.score_samples(X_test)\n",
    "    proba = np.vstack((log_prob_0, log_prob_1)).T\n",
    "    \n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)[:,int(hold_out_rating)]\n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    score = ranking[0] > 89\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dacaff57-b04c-4e07-93c1-4f3e0a52f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_gmm(user_list, n_components, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "\n",
    "    '''Compute hit rate across diferent users for GMM'''\n",
    "    \n",
    "    hit_list = []\n",
    "    for user_id in user_list:\n",
    "        score = run_gmm(user_id, n_components=n_components, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                            movies_data=movies_data, num_rep=num_rep)\n",
    "        hit_list.append(bool(score))\n",
    "    return sum(hit_list) / len(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c3fc18d-eba2-4e09-99a4-20ad0ce48a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cosim(user_id, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Compute two cosine similarity distances for each example in the test set, one against the positive examples in the training\n",
    "    set and other against the negative examples, and use the distances to compute the ranking and return the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # compute the cosine similarities, one for the positive labels and one for the negative labels\n",
    "    cosim_0 = cosine_similarity(X_test, X_train[y_train==0]).sum(axis=1)\n",
    "    cosim_1 = cosine_similarity(X_test, X_train[y_train==1]).sum(axis=1)\n",
    "    distance = np.vstack((cosim_0, cosim_1)).T\n",
    "\n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(distance, axis=0)[:,int(hold_out_rating)]\n",
    "    # apply a positive hit if test example ranked on top-10 in ascending order\n",
    "    score = ranking[0] < 10\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a6c68b8-adc3-4a05-960c-dad6088c86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_cosim(user_list, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "\n",
    "    '''Compute hit rate across diferent users for cosine similarity'''\n",
    "    \n",
    "    hit_list = []\n",
    "    for user_id in user_list:\n",
    "        score = run_cosim(user_id, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                          movies_data=movies_data, num_rep=num_rep)\n",
    "        hit_list.append(bool(score))\n",
    "    return sum(hit_list) / len(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9f380c2-e388-420f-b0e4-f892d022a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(user_id, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Run a random classifier as a baseline'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # assign random probabilities for each class\n",
    "    proba = np.random.uniform(low=0.0, high=1.0, size=100).reshape(100,1)\n",
    "    # compute the ranking for class==test_ratings\n",
    "    ranking = np.argsort(proba, axis=0)\n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    score = ranking[0] > 89\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56bdfaa4-4f55-4bd4-98ea-8955e5ed50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_baseline(user_list, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "\n",
    "    '''Compute hit rate across diferent users for random baseline'''\n",
    "    \n",
    "    hit_list = []\n",
    "    for user_id in user_list:\n",
    "        score = run_baseline(user_id, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                             movies_data=movies_data, num_rep=num_rep)\n",
    "        hit_list.append(bool(score))\n",
    "    return sum(hit_list) / len(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5c1a766-875f-4231-922a-5924ca466434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(user_id, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "    \n",
    "    '''Run all classifiers for a single user and compute the average probability before returning the score'''\n",
    "    \n",
    "    # get movies and ratings from user training data\n",
    "    train_movies = train_ratings_data[train_ratings_data['userId']==user_id]['imdb_id']\n",
    "    train_ratings = train_ratings_data[train_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movies in training set.'.format(user_id, len(train_movies)))\n",
    "    #print('User {} gave {} positive ratings and {} negative ratings.'.format(user_id, (train_ratings==1).sum(), (train_ratings==0).sum()), '\\n')\n",
    "    \n",
    "    # get movie and rating for hold out user test data\n",
    "    hold_out_movie = test_ratings_data[test_ratings_data['userId']==user_id]['imdb_id']\n",
    "    hold_out_rating = test_ratings_data[test_ratings_data['userId']==user_id]['rating']\n",
    "    #print('User {} rated {} movie in the hold-out set.'.format(user_id, len(hold_out_movie)))\n",
    "    #print('User {} gave a {} rating for the hold-out movie.'.format(user_id, int(hold_out_rating)), '\\n')\n",
    "    \n",
    "    # complement test data with other 99 randomly selected movies\n",
    "    allowed_list = movies_data.loc[~movies_data.index.isin(train_movies.append(hold_out_movie))]\n",
    "    rd_movies = allowed_list.sample(n=99, replace=False).index.to_series()\n",
    "    test_movies = hold_out_movie.append(rd_movies)\n",
    "    \n",
    "    # extract X_train and X_test matrices\n",
    "    X_train = num_rep[[movies_data.index.get_loc(x) for x in train_movies], :]\n",
    "    #print('X_train shape:', X_train.shape)\n",
    "    X_test = num_rep[[movies_data.index.get_loc(x) for x in test_movies], :]\n",
    "    #print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    # extrac y_train vector\n",
    "    y_train = train_ratings.values\n",
    "    #print('y_train shape:', y_train.shape, '\\n')\n",
    "    \n",
    "    # initialize the models\n",
    "    bnb = BernoulliNB(alpha=0.001)\n",
    "    rfc = RandomForestClassifier(n_estimators=2, criterion='entropy')\n",
    "    svm = SVC(C=1.0, kernel='rbf', probability=True)\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    lr = LogisticRegression(C=0.0100)\n",
    "    gmm_0 = GaussianMixture(n_components=3, covariance_type='full', random_state=100)\n",
    "    gmm_1 = GaussianMixture(n_components=3, covariance_type='full', random_state=100)\n",
    "    \n",
    "    # fit training data\n",
    "    bnb.fit(X_train, y_train)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    svm.fit(X_train, y_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "    gmm_0.fit(X_train[y_train==0])\n",
    "    gmm_1.fit(X_train[y_train==1])\n",
    "    \n",
    "    # compute probabilities for each model for class==test_ratings\n",
    "    bnb_proba = bnb.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    rfc_proba = rfc.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    svm_proba = svm.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    knn_proba = knn.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    lr_proba = lr.predict_proba(X_test)[:,int(hold_out_rating)].reshape(100,1)\n",
    "    gmm_prob_0 = np.exp(gmm_0.score_samples(X_test))\n",
    "    gmm_prob_1 = np.exp(gmm_1.score_samples(X_test))\n",
    "    gmm_proba = np.vstack((gmm_prob_0, gmm_prob_1)).T[:,int(hold_out_rating)].reshape(100,1)\n",
    "    \n",
    "    # compute the average probabilities\n",
    "    proba = np.hstack((bnb_proba, rfc_proba, svm_proba, knn_proba, lr_proba, gmm_proba)).mean(axis=1)\n",
    "    \n",
    "    # compute the ranking\n",
    "    ranking = np.argsort(proba, axis=0)\n",
    "    # apply a positive hit if test example ranked on top-10 in descending order\n",
    "    score = ranking[0] > 89\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef246225-2c66-4554-841f-e08c96cd2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate_ensemble(user_list, train_ratings_data=dev_train, test_ratings_data=dev_test, movies_data=movies_df, num_rep=num_rep):\n",
    "\n",
    "    '''Compute hit rate across diferent users for ensemble method'''\n",
    "    \n",
    "    hit_list = []\n",
    "    for user_id in user_list:\n",
    "        score = run_ensemble(user_id, train_ratings_data=train_ratings_data, test_ratings_data=test_ratings_data, \n",
    "                             movies_data=movies_data, num_rep=num_rep)\n",
    "        hit_list.append(bool(score))\n",
    "    return sum(hit_list) / len(hit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e6bfd-b520-4184-9e3d-458599f35836",
   "metadata": {},
   "source": [
    "#### Test different classifiers using the unreduced movies features, converted to a CSR sparse matrix, and then to a dataframe\n",
    "\n",
    "In this section, we test the hit rates obtained via various classifiers that we've learned this semester, and compare the rates to that obtained by a random classifier. In each classifier test, all movie features are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "831fe004-7ab7-426e-b01c-e89d04054742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "clf_list = ['Base', 'Cosine Sim', 'BNB', 'RF', 'SVM', 'KNN', 'LR', 'GMM', 'Ensemble']\n",
    "clf_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94c74b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random user list of 200 users from the dev set to be tested via all classifiers\n",
    "user_list = np.random.choice(dev_train['userId'].unique(), size=sample_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "287ec8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity of the movies metadata is: 97.27 percent\n"
     ]
    }
   ],
   "source": [
    "# The processed movies dataset has a high degree of sparsity; therefore, we'll use CSR to reduce model runtime (improve space complexity)\n",
    "print('The sparsity of the movies metadata is:',\n",
    "      round((np.size(movies_df)-np.count_nonzero(movies_df))/np.size(movies_df) *100,2),\n",
    "      'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddcda0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the movies dataframe to a csr sparse matrix to reduce classifier run time\n",
    "movie_features_csr = csr_matrix(movies_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df8719-7cbe-413f-912b-ff0b6281a392",
   "metadata": {},
   "source": [
    "#### Baseline (random classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b00672b7-9888-434d-8c05-c31b33f7f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (random classifier)\n",
      "hit rate==0.115\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate\n",
    "hit_rate = get_hit_rate_baseline(user_list, num_rep=movie_features_csr)\n",
    "# print results\n",
    "print('Baseline (random classifier)')\n",
    "print('hit rate=={:.3f}'.format(hit_rate))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(hit_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae1fc6-a6f7-4c0a-ac52-1994f8786016",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14e05fc2-3371-443a-8bdc-aeb0d57b427b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity\n",
      "hit rate==0.095\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate\n",
    "hit_rate = get_hit_rate_cosim(user_list, num_rep=movie_features_csr)\n",
    "# print results\n",
    "print('Cosine similarity')\n",
    "print('hit rate=={:.3f}'.format(hit_rate))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(hit_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5baf416-e9b8-4b69-9e6d-5bc580c5d8b4",
   "metadata": {},
   "source": [
    "#### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7f56743-2da4-4886-b244-8427115f12bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB\n",
      "For alpha==0.0001 hit rate==0.095\n",
      "For alpha==0.0010 hit rate==0.090\n",
      "For alpha==0.0100 hit rate==0.095\n",
      "For alpha==0.1000 hit rate==0.095\n",
      "For alpha==1.0000 hit rate==0.080\n",
      "For alpha==2.0000 hit rate==0.075\n",
      "--------------------------------------------------\n",
      "Best param: 0.0001\n",
      "Best hit rate: 0.095\n"
     ]
    }
   ],
   "source": [
    "# define range for alpha parameter\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1, 2]\n",
    "# for different values of parameter alpha\n",
    "hit_rate_list = []\n",
    "print('BernoulliNB')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    bnb = BernoulliNB(alpha=param)\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate(user_list, num_rep=movie_features_csr, clf=bnb)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For alpha=={:.4f} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_bnb_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.4f}'.format(best_bnb_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad0e34-b6e3-496b-b5a7-ac1146226a98",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeb2f3bc-7357-4d1a-9581-774eec0fc6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "For n_estimators==  1 hit rate==0.130\n",
      "For n_estimators==  2 hit rate==0.160\n",
      "For n_estimators==  5 hit rate==0.170\n",
      "For n_estimators== 10 hit rate==0.100\n",
      "For n_estimators== 50 hit rate==0.080\n",
      "For n_estimators==100 hit rate==0.105\n",
      "--------------------------------------------------\n",
      "Best param:   5\n",
      "Best hit rate: 0.170\n"
     ]
    }
   ],
   "source": [
    "# define range for n_estimators parameter\n",
    "param_range = [1, 2, 5, 10, 50, 100]\n",
    "# for different values of n_estimators parameter\n",
    "hit_rate_list = []\n",
    "print('Random Forest')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    rfc = RandomForestClassifier(n_estimators=param, criterion='entropy')\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate(user_list, num_rep=movie_features_csr, clf=rfc)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For n_estimators=={:3} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_rf_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:3}'.format(best_rf_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a5676-8af8-4895-ae38-0326aed15db9",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "898af55b-b539-4793-9a1c-659981c071c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "For C==0.001 hit rate==0.080\n",
      "For C==0.010 hit rate==0.075\n",
      "For C==0.100 hit rate==0.140\n",
      "For C==1.000 hit rate==0.075\n",
      "For C==10.000 hit rate==0.110\n",
      "--------------------------------------------------\n",
      "Best param: 0.100\n",
      "Best hit rate: 0.140\n"
     ]
    }
   ],
   "source": [
    "# define range for C parameter\n",
    "param_range = [0.001, 0.01, 0.1, 1, 10]\n",
    "# for different values of C parameter\n",
    "hit_rate_list = []\n",
    "print('SVM')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    svm = SVC(C=param, kernel='rbf', probability=True)\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate(user_list, num_rep=movie_features_csr, clf=svm)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For C=={:.3f} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_svm_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.3f}'.format(best_svm_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079dc5de-ee05-4a58-9d54-fa578fb82cd8",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c39cd46b-d104-408f-9298-7a9c5371b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "For k== 5 hit rate==0.140\n",
      "For k==10 hit rate==0.140\n",
      "For k==15 hit rate==0.130\n",
      "For k==20 hit rate==0.120\n",
      "For k==25 hit rate==0.125\n",
      "--------------------------------------------------\n",
      "Best param:  5\n",
      "Best hit rate: 0.140\n"
     ]
    }
   ],
   "source": [
    "# define range for k (n_neighbors) parameter\n",
    "param_range = [5, 10, 15, 20, 25]\n",
    "# for different values of parameter k (n_beighbors)\n",
    "hit_rate_list = []\n",
    "print('K-Nearest Neighbors')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=param)\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate(user_list, num_rep=movie_features_csr, clf=knn)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For k=={:2} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_knn_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:2}'.format(best_knn_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1567233-0683-41e3-9902-4ad033b9d84b",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5ebaa78-36d0-4751-b15b-a2be40013ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "For C==0.0001 hit rate==0.095\n",
      "For C==0.0010 hit rate==0.080\n",
      "For C==0.0100 hit rate==0.095\n",
      "For C==0.1000 hit rate==0.085\n",
      "For C==1.0000 hit rate==0.095\n",
      "--------------------------------------------------\n",
      "Best param: 0.0001\n",
      "Best hit rate: 0.095\n"
     ]
    }
   ],
   "source": [
    "# define range for C parameter\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "# for different values of C parameter\n",
    "hit_rate_list = []\n",
    "print('Logistic Regression')\n",
    "for param in param_range:\n",
    "    # initialize classifier\n",
    "    lr = LogisticRegression(C=param)\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate(user_list, num_rep=movie_features_csr, clf=lr)\n",
    "    \n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For C=={:.4f} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_lr_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {:.4f}'.format(best_lr_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b8c3a-1a80-45b2-8651-2a9f223a8bef",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d5593b6-146a-49a1-b0fe-4dc6b2d25133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM\n",
      "For n_components==1 hit rate==0.090\n",
      "For n_components==2 hit rate==0.105\n",
      "For n_components==3 hit rate==0.100\n",
      "For n_components==4 hit rate==0.090\n",
      "For n_components==5 hit rate==0.105\n",
      "--------------------------------------------------\n",
      "Best param: 2\n",
      "Best hit rate: 0.105\n"
     ]
    }
   ],
   "source": [
    "# define range for n_components parameter\n",
    "param_range = [1, 2, 3, 4, 5]\n",
    "# for different values of n_components parameter\n",
    "hit_rate_list = []\n",
    "print('GMM')\n",
    "for param in param_range:\n",
    "    # compute the hit rate\n",
    "    hit_rate = get_hit_rate_gmm(user_list, num_rep=num_rep, n_components=param)\n",
    "    # append hit rate to the hit_rate_list\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    # print results\n",
    "    print('For n_components=={} hit rate=={:.3f}'.format(param, hit_rate))\n",
    "best_gmm_param = param_range[hit_rate_list.index(max(hit_rate_list))]\n",
    "print('-'*50)\n",
    "print('Best param: {}'.format(best_gmm_param))\n",
    "print('Best hit rate: {:.3f}'.format(max(hit_rate_list)))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(max(hit_rate_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b2400-dfb9-4515-a35e-c4c4ca280222",
   "metadata": {},
   "source": [
    "#### Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfc5cf69-f277-4758-8f98-e2fd613fd7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Learning\n",
      "hit rate==0.115\n"
     ]
    }
   ],
   "source": [
    "# compute the hit rate\n",
    "hit_rate = get_hit_rate_ensemble(user_list, num_rep=num_rep)\n",
    "# print results\n",
    "print('Ensemble Learning')\n",
    "print('hit rate=={:.3f}'.format(hit_rate))\n",
    "# append best_score to clf_scores\n",
    "clf_scores.append(hit_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f674f-7097-421c-bde9-56b0ce1939b8",
   "metadata": {},
   "source": [
    "#### Plot comparative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9638bec3-02eb-418a-8044-6804764cf7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.bar(clf_list[1:], clf_scores[1:], alpha=0.5)\n",
    "ax.hlines(clf_scores[0], xmin=-0.5, xmax=7.5, color='black', linestyles='dashed', label='baseline')\n",
    "ax.set_ylabel('Hit Ratio')\n",
    "ax.set_ylim(0.0, 0.2)\n",
    "ax.set_title('Best Hit Ratio by Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5c8e2",
   "metadata": {},
   "source": [
    "### Step 8: Rerun the best models for several of the higher performing classifiers, but do so using the reduced parameters obtained by PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683dc49",
   "metadata": {},
   "source": [
    "In this section, we'll use the same user list randomly generated above to test the classifiers, using the PCA reduced set of features in place of movies_df. To test the classifiers, we assigned the classifier the parameter with the best hit rate as found on the unreduced data (in theory, the classifiers should perform better on reduced data, due to decreased overfitting). The findings appear to confirm our previous theory that PCA dimensionality reduction is not a good fit - hit rate scores are lower than on unreduced data, even for the highest performing classifers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b3e08",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70ea5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==  5 hit rate on pca-reduced features==0.115\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "rfc = RandomForestClassifier(n_estimators=best_rf_param, criterion='entropy')\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on pca-reduced features=={:.3f}'.format(best_rf_param, pca_hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a43d0f",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61f96499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==0.1 hit rate on pca-reduced features==0.095\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "svm = SVC(C=best_svm_param, kernel='rbf', probability=True)\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on pca-reduced features=={:.3f}'.format(best_svm_param, pca_hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bb7b7",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8096706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==  5 hit rate on pca-reduced features==0.120\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=best_knn_param)\n",
    "\n",
    "# compute the hit rate\n",
    "pca_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=pca_features)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on pca-reduced features=={:.3f}'.format(best_knn_param, pca_hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d4a91",
   "metadata": {},
   "source": [
    "### Step 9: Rerun the best models for several of the higher performing classifiers, but do so using the reduced fetures obtained from feature importance values from a hyperparameter-tuned XGBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57e562",
   "metadata": {},
   "source": [
    "#### Test various classifiers on the dataset of reduced movie features obtained by feature importance values from a hyperparameter-tuned XGBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3f40",
   "metadata": {},
   "source": [
    "Similarly to our findings when testing the classifiers on PCA-reduced features, we also found worse performance when evaluating our models on a dataset with reduced features (those with non-zero importance to the weighted average, selected by XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d0bb7",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "629d2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==  5 hit rate on selected features==0.125\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "rfc = RandomForestClassifier(n_estimators=best_rf_param, criterion='entropy')\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on selected features=={:.3f}'.format(best_rf_param, select_features_hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aed2ee",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f0143c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==0.1 hit rate on selected features==0.120\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "svm = SVC(C=best_svm_param, kernel='rbf', probability=True)\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on selected features=={:.3f}'.format(best_svm_param, select_features_hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda214",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4266790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators==0.1 hit rate on selected features==0.105\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=best_knn_param)\n",
    "\n",
    "# compute the hit rate\n",
    "select_features_hit_rate = get_hit_rate(user_list, clf=rfc, num_rep=xgb_features_csr)\n",
    "    \n",
    "# print results\n",
    "print('For n_estimators=={:3} hit rate on selected features=={:.3f}'.format(best_svm_param, select_features_hit_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
